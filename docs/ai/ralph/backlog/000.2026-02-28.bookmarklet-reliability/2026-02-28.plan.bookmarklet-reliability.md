# Bookmarklet Script Reliability & Correctness Overhaul

## Context

The `js/script.gpt.temp.js` bookmarklet is a ChatGPT-generated rewrite of `js/script.js` that adds resume/pause, batched exports, localStorage persistence, dead-letter queues, change detection, and a built-in ZIP implementation (eliminating the JSZip dependency). It works but has several reliability and correctness issues that need fixing before it can replace the original script.

## Scope

Eight changes, all focused on reliability and correctness:

### 1. IndexedDB Storage (replaces localStorage)

**Problem:** `S.progress.exported` (every exported conversation ID) and `S.scan.snapshot` (every conversation's `[id, update_time]`) grow linearly with account size. localStorage has a ~5MB limit. For users with thousands of conversations, `Store.save()` silently fails (catches with `console.warn`), meaning resume state is lost without the user knowing.

**Solution:**
- Replace `localStorage` with IndexedDB via a thin async wrapper (~30 lines)
- Single object store, key-value style: one key `"state"`, value = full `S` object
- `Store.load()` becomes `await Store.load()` — boot sequence wraps in async init
- `Store.save()` becomes async internally but stays fire-and-forget (debounced as today)
- Fallback: if IndexedDB is unavailable (rare — some private browsing modes), fall back to localStorage with a visible warning in the UI
- Schema: single blob per state save (no splitting into separate records)

### 2. Revert Scan Pagination (decouple from batch setting)

**Problem:** `scanConversations` was changed to use `S.settings.batch` as the API page size. Scan pagination and export batch size are different concepts. If the user sets batch=5 for small careful exports, scanning 3500+ conversations would take 700+ API calls instead of 70.

**Solution:**
- `scanConversations` gets its own hardcoded `pageSize = 50` for API pagination
- `S.settings.batch` only controls export batch size in `exportOneBatch`
- Two different concepts, two different values

### 3. Unified Fetch Core (DRY refactor)

**Problem:** `fetchJson` and `fetchBlob` are ~90% copy-pasted code. Both implement auth, 401 refresh, 429 backoff, and 5xx retries independently. A bug fixed in one but not the other creates silent inconsistency.

**Solution:**
- Extract a shared `_fetch(url, opts)` method that handles:
  - Auth header injection
  - 401 token refresh (1 retry)
  - 429 exponential backoff with jitter
  - 5xx retry (up to 3 attempts)
- `fetchJson(url, opts)` = `_fetch(url, opts)` then `resp.json()`
- `fetchBlob(url, opts)` = `_fetch(url, opts)` then `resp.blob()`
- One retry/backoff implementation, no drift

### 4. 429 Circuit Breaker

**Problem:** If the API permanently rate-limits the user, the script retries forever with exponential backoff. No circuit breaker.

**Solution:**
- Add a max retry count of 10 consecutive 429s with no successful request in between
- At the current backoff formula (`10000 * 1.6^count`, capped at 120s, with jitter), this is ~12 minutes of retrying
- After 10 retries, throw: `"Rate limit exceeded after 10 retries — try again later"`
- Reset the counter on any successful request

### 5. Token Refresh Deduplication

**Problem:** Multiple concurrent workers can call `Net.getToken()` simultaneously, causing redundant auth requests and unpredictable state.

**Solution:**
- Guard `getToken` with a pending promise:
  ```js
  if (this._tokenPromise) return this._tokenPromise;
  this._tokenPromise = this._doGetToken(signal)
    .finally(() => { this._tokenPromise = null; });
  return this._tokenPromise;
  ```
- If a refresh is already in flight, all callers await the same promise

### 6. Dead Code Removal

**Problem:** `const R = window.__CVZ_RESUME__ = window.__CVZ_RESUME__ || {};` (line 3) is assigned but never referenced anywhere.

**Solution:** Remove it.

### 7. Replace `script.js`

**Action:**
- The fixed bookmarklet becomes the new `js/script.js`
- Delete `js/script.gpt.temp.js`

### 8. Update `HOW_TO_USE.md`

**Changes:**
- Remove the "Load JSZip" step (the script now has a built-in `ZipLite` class that creates uncompressed ZIPs — no external dependency needed)
- Document the new workflow: bookmarklet click -> UI with resume/pause, batch settings, progress stats
- Document resume behavior: state persists across page reloads and browser restarts via IndexedDB
- Document batch size control: stop -> change -> start

## Why JSZip Is No Longer Needed

The new script includes a `ZipLite` class (~115 lines) that creates ZIP files from scratch using raw byte manipulation (CRC32, local headers, central directory). Files are stored uncompressed (ZIP method 0 / STORE), which means slightly larger ZIPs — but in practice this barely matters because media files (JPEG, PNG, WebP) are already compressed and deflate would save almost nothing. The trade-off is a fully self-contained bookmarklet with no setup step.

## Non-Goals

- **Stale export detection** (re-exporting updated conversations): feature addition, not a bug fix. Can be addressed separately.
- **UI redesign**: the current UI is functional. Bookmarklet constraints make elaborate UI impractical.
- **Compression in ZipLite**: diminishing returns for the added code size.
